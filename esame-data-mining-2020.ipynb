{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real or Not? NLP with Disaster Tweets\n",
    "*Gruppo:\n",
    "Lorrai, Rossi*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Collecting pyspellchecker\r\n",
      "  Downloading pyspellchecker-0.5.4-py2.py3-none-any.whl (1.9 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 2.8 MB/s \r\n",
      "\u001b[?25hInstalling collected packages: pyspellchecker\r\n",
      "Successfully installed pyspellchecker-0.5.4\r\n",
      "/kaggle/input/nlp-getting-started/sample_submission.csv\n",
      "/kaggle/input/nlp-getting-started/test.csv\n",
      "/kaggle/input/nlp-getting-started/train.csv\n",
      "\n",
      "Import Completati\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import linear_model\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from statistics import mean\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "!pip install pyspellchecker\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        \n",
    "print(\"\\nImport Completati\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminiamo eventuali tag -> Input \"<title>titolo</title> : Output \"titolo\"\n",
    "def cleanhtml(text):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', text)\n",
    "    return cleantext\n",
    "\n",
    "#Rimuoviamo i text contenenti solamente dei link, non sono significativi -> Input \"https://link_al_sito.com\" : Output \"\"\n",
    "def removeurl(text):\n",
    "    clean_text = re.sub(r'^http?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    return clean_text\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "           \n",
    "stop=set(stopwords.words('english') + [\"http\",\"https\", \"s\", \"nt\", \"m\"] )\n",
    "\n",
    "def adjust_spelling(text):\n",
    "    if text is not None:\n",
    "        tokens = [spell.correction(x) for x in (word_tokenize(text))]\n",
    "        return \" \".join(tokens)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    if text is not None:\n",
    "        tokens = [x for x in word_tokenize(text) if x not in stop]\n",
    "        return \" \".join(tokens)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def lemmatize_words(text):\n",
    "    if text is not None:\n",
    "        tokens = [lemmatizer.lemmatize(x) for x in (word_tokenize(text))]\n",
    "        return \" \".join(tokens)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# ho tolto . \n",
    "def handle_punct_1(text):\n",
    "    # punctuations\n",
    "    punctuations = '@#!?+&*[]-%/()$=><|{}^:,;-' + \"'`\"\n",
    "    for p in punctuations:\n",
    "        #text = text.replace(p, f' {p} ')\n",
    "        text = text.replace(p, f' ')\n",
    "    return text\n",
    "\n",
    "# maneggio .  \n",
    "def handle_punct_2(text):\n",
    "    clean_text = re.sub(r'\\.\\s', ' ', text, flags=re.MULTILINE)\n",
    "    #clean_text = re.sub(r'\\.$', ' ', text, flags=re.MULTILINE)\n",
    "    return clean_text\n",
    "def handle_punct_3(text):\n",
    "    clean_text = re.sub(r'\\.$', ' ', text, flags=re.MULTILINE)\n",
    "    return clean_text\n",
    "\n",
    "# sistema ... separandolo\n",
    "def handle_punct_4(text):\n",
    "    # ... and ..\n",
    "    #text = text.replace(' . ', ' pippo ')\n",
    "    text = text.replace('...', ' ... ')\n",
    "    if '...' not in text:\n",
    "        text = text.replace('..', ' ... ')\n",
    "    elif '..' not in text: \n",
    "        text = text.replace('.', ' ... ')\n",
    "    return text\n",
    "\n",
    "def handle_char_entity(text):\n",
    "    text = re.sub(r\"&gt;\", \">\", text)\n",
    "    text = re.sub(r\"&lt;\", \"<\", text)\n",
    "    text = re.sub(r\"&amp;\", \"&\", text)\n",
    "    return text\n",
    "\n",
    "def remove_multi_whitespaces(text):\n",
    "    clean_text = re.sub(r'\\s+', ' ', text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(table):\n",
    "    \n",
    "    # lower --> perdiamo i title!!!\n",
    "    table['text'] = table['text'].apply(lambda x : x.lower())\n",
    "\n",
    "    # clean html\n",
    "    table['text'] = table['text'].apply(lambda x:cleanhtml(x))\n",
    "\n",
    "    # remove single url tweet\n",
    "    table['text'] = table['text'].apply(lambda x:removeurl(x))\n",
    "\n",
    "    # remove emoji\n",
    "    table['text'] = table['text'].apply(lambda x: remove_emoji(x))\n",
    "\n",
    "    # vedi singole funzioni // migliore: rimuove punto dalle parole, separa punteggiatura\n",
    "    table['text'] = table['text'].apply(lambda x : handle_punct_1(x))\n",
    "    table['text'] = table['text'].apply(lambda x : handle_punct_2(x))\n",
    "    table['text'] = table['text'].apply(lambda x : handle_punct_3(x))\n",
    "    table['text'] = table['text'].apply(lambda x : handle_punct_4(x))\n",
    "\n",
    "    # sistema &amp e altro con i char veri\n",
    "    table['text'] = table['text'].apply(lambda x : handle_char_entity(x))\n",
    "        \n",
    "    # aggiusta spelling\n",
    "    #table['text'] = table['text'].apply(lambda x: adjust_spelling(x))\n",
    "    \n",
    "    print(\"done with adjusting\")\n",
    "    \n",
    "    # crea lemmi\n",
    "    table['text'] = table['text'].apply(lambda x: lemmatize_words(x))\n",
    "\n",
    "    # rimuove stopwords\n",
    "    table['text'] = table['text'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "    # rimuove multi spazi bianchi\n",
    "    table['text'] = table['text'].apply(lambda x : remove_multi_whitespaces(x))\n",
    "    \n",
    "    print(\"done\")\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\n",
    "train_data = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\n",
    "\n",
    "train_data.dropna(axis=0, subset=['target'], inplace=True)\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "",
    "_uuid": ""
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with adjusting\n",
      "done\n",
      "done with adjusting\n",
      "done\n",
      "setup complete\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deed reason earthquake may allah forgive u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13 000 people receive wildfire evacuation orde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>two giant crane holding bridge collapse nearby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aria_ahrary thetawniest control wild fire cali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m1.94 01 04 utc 5km volcano hawaii t.co zdtoyd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>police investigating e bike collided car littl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>latest home razed northern california wildfire...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  \n",
       "0            deed reason earthquake may allah forgive u  \n",
       "1                 forest fire near la ronge sask canada  \n",
       "2     resident asked shelter place notified officer ...  \n",
       "3     13 000 people receive wildfire evacuation orde...  \n",
       "4     got sent photo ruby alaska smoke wildfire pour...  \n",
       "...                                                 ...  \n",
       "7608  two giant crane holding bridge collapse nearby...  \n",
       "7609  aria_ahrary thetawniest control wild fire cali...  \n",
       "7610  m1.94 01 04 utc 5km volcano hawaii t.co zdtoyd...  \n",
       "7611  police investigating e bike collided car littl...  \n",
       "7612  latest home razed northern california wildfire...  \n",
       "\n",
       "[7613 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Separo gli output y dagli input x\n",
    "train_y = train_data[\"target\"].values\n",
    "train_data.drop(['target'], axis=1, inplace=True)\n",
    "\n",
    "## Pulisco i dataset di train e test\n",
    "test_data = clean_text(test_data)\n",
    "train_data = clean_text(train_data)\n",
    "\n",
    "print('setup complete')\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_transformation(features=None, ngrams=(1,1)):\n",
    "    tfidf_vec = TfidfVectorizer(stop_words='english', max_features=features, ngram_range=ngrams)\n",
    "    tfidf_vec.fit_transform(train_data['text'].values.tolist() + test_data['text'].values.tolist())\n",
    "\n",
    "    train_tfidf = tfidf_vec.transform(train_data['text'].values.tolist())\n",
    "    test_tfidf = tfidf_vec.transform(test_data['text'].values.tolist())\n",
    "\n",
    "    return train_tfidf, test_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creazione del Modello - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes, linear_model\n",
    " \n",
    "## Definisce un modello, fitta, e predice\n",
    "def runModel(model, train_X, train_y, test_X):\n",
    "    #model = linear_model.LogisticRegression(C=1, solver='sag')\n",
    "    model.fit(train_X, train_y)\n",
    "    preds = model.predict_proba(test_X)[:,1]\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if __name__ == \\'__main__\\':\\n    \\n    params_used = []\\n    \\n    \\n    for num_features in [None, 3000, 8000, 15000]:\\n        for num_grams in [(1,1), (1,2)]:\\n            train_tfidf, test_tfidf = tf_idf_transformation(num_features, num_grams)\\n\\n            #1, 1.5, 2.5, 5, 10, 15\\n            C_list = [0.1, 0.5, 1, 1.5, 2.5, 5, 10, 15]\\n\\n            #usare 10\\n            num_splits = 10\\n\\n            for c in C_list:\\n                \\n                param_with_score = {\\n                    \"n_features\" : None,\\n                    \"n_grams\" : (1,1),\\n                    \"c\": 0.1,\\n                    \"score\": 0,\\n                    \"threshold\": 0,\\n                }\\n\\n                param_with_score[\"c\"] = c\\n                param_with_score[\"n_grams\"] = num_grams\\n                param_with_score[\"n_features\"] = num_features\\n\\n                model = linear_model.LogisticRegression(C=c, solver=\\'sag\\')\\n                best_threshold = 0\\n                best_score = 0\\n\\n                #usare 0.01 e range(100)\\n                for threshold in tqdm([i * 0.01 for i in range(100)]):\\n                    score = 0\\n                    kf = model_selection.KFold(n_splits=num_splits, shuffle=True, random_state = 0)\\n                    for train_index, val_index in kf.split(train_tfidf):\\n                        X_train, X_val = train_tfidf[train_index], train_tfidf[val_index]\\n                        y_train, y_val = train_y[train_index], train_y[val_index]\\n\\n                        preds = runModel(model, X_train, y_train, X_val)\\n                        score += metrics.f1_score(y_true = y_val, y_pred = preds > threshold)\\n\\n                    score /= num_splits\\n\\n                    if score > best_score:\\n                        best_threshold = threshold\\n                        best_score = score\\n\\n                param_with_score[\"score\"] = best_score\\n                param_with_score[\"threshold\"] = best_threshold\\n\\n                params_used.append(param_with_score)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''if __name__ == '__main__':\n",
    "    \n",
    "    params_used = []\n",
    "    \n",
    "    \n",
    "    for num_features in [None, 3000, 8000, 15000]:\n",
    "        for num_grams in [(1,1), (1,2)]:\n",
    "            train_tfidf, test_tfidf = tf_idf_transformation(num_features, num_grams)\n",
    "\n",
    "            #1, 1.5, 2.5, 5, 10, 15\n",
    "            C_list = [0.1, 0.5, 1, 1.5, 2.5, 5, 10, 15]\n",
    "\n",
    "            #usare 10\n",
    "            num_splits = 10\n",
    "\n",
    "            for c in C_list:\n",
    "                \n",
    "                param_with_score = {\n",
    "                    \"n_features\" : None,\n",
    "                    \"n_grams\" : (1,1),\n",
    "                    \"c\": 0.1,\n",
    "                    \"score\": 0,\n",
    "                    \"threshold\": 0,\n",
    "                }\n",
    "\n",
    "                param_with_score[\"c\"] = c\n",
    "                param_with_score[\"n_grams\"] = num_grams\n",
    "                param_with_score[\"n_features\"] = num_features\n",
    "\n",
    "                model = linear_model.LogisticRegression(C=c, solver='sag')\n",
    "                best_threshold = 0\n",
    "                best_score = 0\n",
    "\n",
    "                #usare 0.01 e range(100)\n",
    "                for threshold in tqdm([i * 0.01 for i in range(100)]):\n",
    "                    score = 0\n",
    "                    kf = model_selection.KFold(n_splits=num_splits, shuffle=True, random_state = 0)\n",
    "                    for train_index, val_index in kf.split(train_tfidf):\n",
    "                        X_train, X_val = train_tfidf[train_index], train_tfidf[val_index]\n",
    "                        y_train, y_val = train_y[train_index], train_y[val_index]\n",
    "\n",
    "                        preds = runModel(model, X_train, y_train, X_val)\n",
    "                        score += metrics.f1_score(y_true = y_val, y_pred = preds > threshold)\n",
    "\n",
    "                    score /= num_splits\n",
    "\n",
    "                    if score > best_score:\n",
    "                        best_threshold = threshold\n",
    "                        best_score = score\n",
    "\n",
    "                param_with_score[\"score\"] = best_score\n",
    "                param_with_score[\"threshold\"] = best_threshold\n",
    "\n",
    "                params_used.append(param_with_score)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for i in range(0, len(params_used)):\\n    print(\"n_features:\", params_used[i][\"n_features\"], \"          n_grams:\", params_used[i][\"n_grams\"], \"C:\", params_used[i][\"c\"], \"          score:\", params_used[i][\"score\"], \"          threshold:\", params_used[i][\"threshold\"])'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for i in range(0, len(params_used)):\n",
    "    print(\"n_features:\", params_used[i][\"n_features\"], \"          n_grams:\", params_used[i][\"n_grams\"], \"C:\", params_used[i][\"c\"], \"          score:\", params_used[i][\"score\"], \"          threshold:\", params_used[i][\"threshold\"])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'index = 0\\n\\nfor i in range(0, len(params_used)):\\n    if (params_used[i][\"score\"] > params_used[index][\"score\"]):\\n        index = i\\n        \\nbest_params = params_used[index]\\n\\nbest_params'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''index = 0\n",
    "\n",
    "for i in range(0, len(params_used)):\n",
    "    if (params_used[i][\"score\"] > params_used[index][\"score\"]):\n",
    "        index = i\n",
    "        \n",
    "best_params = params_used[index]\n",
    "\n",
    "best_params'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_features': 8000,\n",
       " 'n_grams': (1, 1),\n",
       " 'c': 1.5,\n",
       " 'score': 0.7603229884729064,\n",
       " 'threshold': 0.43}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = {\n",
    "    \"n_features\": 8000,\n",
    "    \"n_grams\": (1,1),\n",
    "    \"c\" : 1.5,\n",
    "    \"score\" : 0.7603229884729064,\n",
    "    \"threshold\" : 0.43,\n",
    "}\n",
    "\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creo il modello, fitto, e predico i valori di test\n",
    "\n",
    "best_train_tfidf, best_test_tfidf = tf_idf_transformation(best_params[\"n_features\"], best_params[\"n_grams\"])\n",
    "\n",
    "model = linear_model.LogisticRegression(C=best_params[\"c\"], solver='sag')\n",
    "model.fit(best_train_tfidf, train_y)\n",
    "pred_test = model.predict_proba(best_test_tfidf)[:,1]\n",
    "\n",
    "\n",
    "final_preds = [1 if x > best_params[\"threshold\"] else 0 for x in pred_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'id': test_data['id'],\n",
    "                       'target': final_preds})\n",
    "output.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primo approccio (Preprocessing) - Aggiungiamo Features Extra\n",
    "Aggiungiamo delle Feature extra derivanti da quelle del dataset originale in modo da migliorare il modello finale\n",
    "\n",
    "- num_word\n",
    "- num_unique_word\n",
    "- num_chars\n",
    "- num_ stop_words (parole ignorate dai search engine: congiunzioni, parole molto frequenti e irrilevanti es: 'for' e 'of')\n",
    "- num_punctuacion\n",
    "- num_words_upper\n",
    "- num_words_title (Programmazione a Numeri Interi : nel titolo di solito le parole importanti sono in maiusc)\n",
    "- mean_word_len\n",
    "- url_count\n",
    "- hashtag_count\n",
    "- mention_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pulisco i dati dai tag html e text contenenti solo URL\n",
    "\n",
    "def add_feature_pre_clean(table):\n",
    "    ## Number of characters in the text ##\n",
    "    table[\"num_chars\"] = table[\"text\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "    ## Number of stopwords in the text ##\n",
    "    #table[\"num_stopwords\"] = table[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords.tolist()]))\n",
    "\n",
    "    ## Number of punctuations in the text ##\n",
    "    #table[\"num_punctuations\"] = table['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "\n",
    "    ## Number of title case words in the text ##\n",
    "    table[\"num_words_upper\"] = table[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "    ## Number of title case words in the text ##\n",
    "    table[\"num_words_title\"] = table[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "    ## Average length of the words in the text ##\n",
    "    table[\"mean_word_len\"] = table[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "\n",
    "    # url_count\n",
    "    table['url_count'] = table['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n",
    "\n",
    "    # hashtag_count\n",
    "    table['hashtag_count'] = table['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
    "\n",
    "    # mention_count\n",
    "    table['mention_count'] = table['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\n",
    "\n",
    "    return table\n",
    "\n",
    "def add_feature_post_clean(table):\n",
    "    ## Number of words in the text ##\n",
    "    table[\"num_words\"] = table[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "    ## Number of unique words in the text ##\n",
    "    table[\"num_unique_words\"] = table[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "    return table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
